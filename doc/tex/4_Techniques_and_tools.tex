\capitulo{4}{Techniques and tools}

\nonzeroparskip In this section are presented the methodological techniques and development tools used to carry out the project.

\section{GitHub}

\nonzeroparskip GitHub~\footnote{\url{https://github.com/}} is a collaborative development platform created in 2008 and based on the Git version control system. Github has a freemium model and provides the ability to host both public and private projects, focusing primarily on code development~\cite{wikipedia_github}.

\nonzeroparskip GitHub is the repository in which the project has been managed and hosted, and Git is the version control through which the commits have been made from the local environment. For the agile methodology, the Milestones have been used as sprints and the Issues as the tasks to be carried out in each of them.

\section{Postman}

\nonzeroparskip Postman~\footnote{\url{https://www.postman.com/}} is a tool that allows the user to build and use APIs in a simple way. Some of its characteristics are the API repository (easier storage, cataloging and collaboration), the availability of tools to help in the API design, testing and documentation, the workspaces to organize the work, and built-in integrations with tools such as GitHub, Azure DevOps, Jenkins, Splunk, Slack and Microsoft Teams. In addition, Postman is based on open source technologies, which provides the ability to be easily extended~\cite{postman}.

\section{Apache Airflow}

\nonzeroparskip Apache Airflow~\footnote{\url{https://airflow.apache.org/}} is a service orchestrator that allows you to plan, manage, and monitor workflows~\cite{airflow}. It was created in 2014 by Airbnb with the aim of handling the company's huge data flows, and published in 2015 under an open source license. In March 2016 the project joined the Apache Software Foundations incubator and was published as a top level project in 2019.

\nonzeroparskip Airflow is used to automate jobs by breaking them down into smaller tasks. For example, this project uses Airflow to automate the ETL process that consumes data from Twitter and Spotify, processes it, and serves it to the user. Among the main features of Airflow are scalability and ease of integration with other tools.

\nonzeroparskip The main element used by Airflow are the Directed Acyclic Graphs or DAGs, which are groups of tasks connected to each other through dependencies like the nodes of a graph. The word \textit{direct} indicates that the existing relationships in the graph must only have one direction (bidirectional relationships between nodes or tasks are not allowed), while the word \textit{acyclic} means that cycles cannot exist in the graph (nodes or tasks cannot be executed more than once). Tasks are defined by means of an operator and there is a very extensive library with operators that allow defining a wide variety of services such as BashOperator, to execute Bash commands, or SparkSubmitOperator, to submit a task to Spark. Regarding the programming language, Python is the one in which DAGs are developed.

\nonzeroparskip Airflow allows you to have control of the tasks executed through a record of their executions, the time, the current or final status and the generated logs. In addition, it allows certain parameters to be associated with each task, such as, for example, the maximum execution time allowed.

\section{Apache Spark}

\nonzeroparskip Apache Spark~\footnote{\url{https://spark.apache.org/}} is a multi-language engine that emerged in 2009 at the University of California used for data processing and machine learning designed for both simple single-node and distributed architectures~\cite{infoworld_spark}. Apache Spark supports the Python, Scala, SQL, Java and R programming languages and allows to design pipelines for large volumes of data for batch and streaming processing. Currently, Spark is one of the most widely used large-scale data processing frameworks.

\nonzeroparskip Key features of Apache Spark include its ability to scale, its speed, and the multitude of free resources made available by the large Apache community. In addition, Apache Spark has been adapted to the main cloud solutions, such as Databricks, Amazon Web Services, Google Cloud Platform and Microsoft Azure.

\nonzeroparskip The Apache Spark architecture is mainly composed of:
\begin{itemize}
	\item \textbf{Driver.} Turn code into tasks to distribute to worker nodes. This code goes on to form DAGs, which indicate the order of the tasks and the node in which they are going to be executed.
	\item \textbf{Executors.} They run on worker nodes and execute assigned tasks.
\end{itemize}

\nonzeroparskip Finally, one of the most characteristic elements of Spark is the Resilient Distributed Dataset or RDD. RDDs are abstractions that represent a collection of immutable objects that can be divided among the different nodes of a cluster and executed in a distributed way.

\section{Cassandra}

\nonzeroparskip Apache Cassandra~\footnote{\url{https://cassandra.apache.org/_/index.html}} is a widely used open source non-relational database that emerged in 2008 as a Google code open source project and in March 2010 as an Apache top level project. It is based on a system that acts as a mix between key-value and column-oriented and introduces Cassandra Query Language or CQL, an alternative query language to SQL with a similar syntax~\cite{cassandra,wikipedia_cassandra}.

\nonzeroparskip Among its main features are:
\begin{itemize}
	\item \textbf{Scalability}. Its performance increases linearly by adding new nodes, which can be added in real time without affecting system performance.
	\item \textbf{Fault tolerance}. Due to its distributed system architecture, data is automatically replicated across multiple nodes and enables replication and redundancy to be provided across multiple datacenters. Cassandra works with partitions, where each node owns a set of tokens whose ranges determine what data to send to each node within the cluster.
	\item \textbf{Decentralization}. All nodes in the cluster are assigned the same role, with no master node that can act as a single point of failure.
	\item \textbf{Consistency}. Considering the CAP theorem (Consistency-Availability-Partition tolerance), Cassandra is built to be AP, that is, to sacrifice consistency to ensure continuous operation without failures. Cassandra allows you to adjust this level of consistency by selecting the minimum number of nodes that must validate a read or write operation before it is considered successful.
\end{itemize}

\nonzeroparskip Cassandra was used in the project as the main database to store Twitter and Spotify data.

\section{Flask}

\nonzeroparskip Flask~\footnote{\url{https://flask.palletsprojects.com/en/2.1.x/}} is...

\section{Bootstrap}

\nonzeroparskip Bootstrap~\footnote{\url{https://getbootstrap.com/}} is...

\section{Docker}

\nonzeroparskip Docker~\footnote{\url{https://www.docker.com/}} is an open-source tool that emerged in 2013 that allows you to add an abstraction layer to the deployment of applications by automating them in a container from software~\cite{docker}. Through Docker, software development can be simplified and applications can be executed in different work environments more easily.

\nonzeroparskip The containers raised by Docker are associated with an image. To build an image in Docker, a file called Dockerfile with a specific syntax is used. Once built, the image will contain the Dockerfile along with the libraries and code specified, and any desired containers can be launched from it.

\nonzeroparskip Docker allows integration with tools such as Ansible and Jenkins, and with cloud providers such as Microsoft Azure, Google Cloud Platform and Amazon Web Services, among others.

\section{Docker Compose}

\nonzeroparskip Docker Compose~\footnote{\url{https://docs.docker.com/compose/}} is a solution that allows you to manage multiple Docker containers on a single host machine~\cite{docker_compose}.

\nonzeroparskip Docker Compose uses a yaml file to indicate the services you want to build along with the images each one should be based on. In this file you can indicate environment variables, dependencies, ports and even commands that must be launched at startup.

\nonzeroparskip While Docker Compose is the Docker container management solution on a single machine, Docker Swarm is the orchestration tool for managing containers on multi-machine distributed architectures.
