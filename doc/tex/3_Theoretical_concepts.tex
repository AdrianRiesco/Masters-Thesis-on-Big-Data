\capitulo{3}{Theoretical concepts}

\nonzeroparskip In this section are covered the theoretical concepts in which the project has been based. All concepts are described in a detailed and simple way since this master's thesis can be aimed at technical and non-technical students.

\section{Big Data}

\nonzeroparskip The term Big Data is used to refer to data sets whose size exceeds the capabilities of the software systems used to capture, preserve and process the data within an acceptable time window.

\nonzeroparskip The definition of Big Data is usually linked to the three dimensions or Vs defined by Doug Laney in 2001~\cite{bigdata_vs}:
\begin{itemize}
	\item \textbf{Volume.} The amount of data to handle.
	\item \textbf{Velocity.} The rate at which new data is created and consumed.
	\item \textbf{Variety.} The diversity or form of the captured data.
\end{itemize}

\nonzeroparskip After the definition of the three Vs, more dimensions have been added: Veracity, Value, Validity, Vagueness, Volatility... Up to a total of 42 different dimensions to define Big Data.

\nonzeroparskip From the moment big data is discovered until it can be used and generate value, it goes through a life cycle made up of the following phases: discovery, raw data ingestion, raw data processing, storage, integration with other data, analysis, and presentation of results.

\nonzeroparskip Due to the complexity of big data, it usually requires a complex and varied technological ecosystem to be managed.

\section{ETL}
\nonzeroparskip An ETL process refers to the stages that every data processing exercise usually goes through:
\begin{enumerate}
	\item \textbf{Extract.} Raw data is collected from the source and fed into our Data Lake, which preserves it in its original form.
	\item \textbf{Transform.} The raw data is cleaned and transformed to be used in our environment.
	\item \textbf{Load.} Once structured and filtered, the data is entered into our Data Warehouse.
\end{enumerate}

\section{API}

\nonzeroparskip An Application Programming Interface or API is an interface that defines the interactions that can be made with a software system. The APIs generally define the data that can be requested and sent to the system, the way to authenticate to it and the format of the returned data~\cite{ibm_restapi}.

\nonzeroparskip In relation to web development, most of the APIs work according to Hypertext Transfer Protocol (HTTP), a communication protocol that allows information transfers through files on the World Wide Web. Additionally and not exclusive, a large number of APIs are developed according to the REST architectural style, defined by Roy Fielding in the year 2000 and which is based on a series of principles that seek to facilitate development:
\begin{enumerate}
	\item Uniform interface for all resources, forcing all queries made to the same resource (each with a specific Uniform Resource Identifier or URI) to have the same form regardless of the origin of the request.
	\item Decoupling between the client and the server, making the only information that the client must know about the server is its identifier (URI) and that the only action to be carried out by the server is to return the data required in the request.
	\item Stateless queries, meaning that each request must contain all the information necessary to be processed without requiring an additional request or storing any type of state.
	\item Allow, whenever possible, both client-side and server-side caching to reduce the load of the former and increase the scalability of the latter.
	\item Layer system, allowing multiple intermediaries between the client and the server and preventing them from knowing in any case if they are communicating with the other party or with an intermediary.
	\item Although the resources exchanged are usually static, a REST architecture can optionally have responses that contain snippets of executable code.
\end{enumerate}

\nonzeroparskip In general terms, an API based on a REST architecture serves to make it easier for developers to develop applications that interact with the resources published by it.

\section{Orchestrator}

\nonzeroparskip The great variety of applications and services that exist in technological environments, where there are workflows with various actors with interdependencies between them, make their management and automation enormously complex. The more complex a system is, the more difficult it is to manage the intervening factors~\cite{redhat_orchestrator}.

\nonzeroparskip System automation usually improves efficiency, simplifies management, and reduces associated costs, both in terms of time spent and personnel required to control it. On the other hand, a distinction is made between automation and orchestration in that the former refers to a single task, while the latter comprises multi-step processes and workflows, being the scope of work of the orchestrators.

\nonzeroparskip Two main orchestrators have been used in this project: Docker Compose, which acts as an orchestrator for the work environment containers, and Apache Airflow, which orchestrates the project's workflow.

\section{NoSQL Databases}

\nonzeroparskip A database is a set of data belonging to the same context and stored for later use, and can be updated periodically. The best known type of databases are relational databases. In a relational database, the data attributes are stored in the form of columns, previously defined, and the values are stored in the rows of the table for all its columns or attributes. These databases have an associated query language called SQL (Structured Query Language).

\nonzeroparskip Relational database properties are summarized in ACID properties:
\begin{itemize}
	\item \textbf{A}tomicity. The process is done completely or it is not done.
	\item \textbf{C}onsistency. Only valid data is written.
	\item \textbf{I}solation. The operations are performed one at a time.
	\item \textbf{D}urability. When an operation is performed, it persists and is not undone even if the system crashes.
\end{itemize}

\nonzeroparskip However, in the face of the massive volumes of data that are associated with the concept of Big Data, relational databases have a series of limitations:
\begin{itemize}
	\item Reading the data is expensive, since the data is represented in tables, queries involve joining large data sets and filtering the results.
	\item The stored information usually has similar structures, a concept that does not agree well with Big Data, where the variety of data structure is greater.
	\item Scalability is not their strongest factor, since they were initially designed considering a single server or, at most, having replicas and load balancing.
\end{itemize}

\nonzeroparskip Distributed databases are limited by the CAP theorem:
\begin{itemize}
	\item \textbf{C}onsistency. The information remains coherent and consistent after any operation, with all copies having the same data at all times.
	\item \textbf{A}vailability. The system continues to function even if any of its nodes or parts of the software or hardware fail, and all reads and writes complete successfully.
	\item \textbf{P}artition tolerance. The system nodes will continue to function even if the connection between them fails or messages are lost, maintaining their properties.
\end{itemize}

\nonzeroparskip According to CAP's theorem, any distributed database with shared data among its nodes can have at most two of the three properties at the same time. This theorem resulted in databases with relaxed ACID properties, that is, with BASE properties:
\begin{itemize}
	\item \textbf{B}asically \textbf{A}vailable: The store works most of the time, even if failures occur.
	\item \textbf{S}oft-State: Stores or their replicas do not have to be consistent at all times.
	\item \textbf{E}ventually Consistent: consistency happens eventually, as it is something that is taken for granted at some point in the future. All copies will gradually become consistent if no further updates are run.
\end{itemize}

\nonzeroparskip Non-relational databases or NoSQL (Not Only SQL), a term introduced by Carl Strozzi in 1998 that describes all those databases that do not follow the same design patterns as relational databases. Non-relational databases follow the BASE properties and have advantages such as:
\begin{itemize}
	\item They do not require a fixed data schema.
	\item The data is replicated on multiple similar nodes and can be partitioned.
	\item They are horizontally scalable, that is, by adding new nodes.
	\item They are relatively inexpensive and simple to implement, with a host of open source alternatives.
	\item They provide fast read and write speeds, with fast key-value access.
\end{itemize}

\nonzeroparskip However, the main disadvantages of non-relational databases is that they do not support certain features of relational databases (join, group by, order by...) except within their partitions, they do not have a query language standard such as SQL and its relaxed ACID or BASE properties give lesser guarantees.

\nonzeroparskip Non-relational databases are mainly divided into four groups:
\begin{itemize}
	\item \textbf{Key/value}. Their data model is very simple, since they only store keys and values. They are very similar to a hash table, they are fast, they have great ease of scaling, eventual consistency and fault tolerance, although they cannot support complex data structures.
	\item \textbf{Column oriented}. Data is stored in columns instead of rows. The data is semi-structured, easily distributable, provides high reading speed, calculations on attributes are faster (especially aggregations such as averages) and are perfect when you want to do many operations on large data sets, but they are not the most efficient for writing or when you want to retrieve all records. The data model has columns, super columns, column families, and super column families.
	\item \textbf{Document oriented}. These are key-value stores in which the value is stored as a document with a defined format, so the final data model is collections of documents with a key-value structure (JSON, PDF, XML...). They are schema-less, highly scalable, programmer-friendly, and support rapid development.
	\item \textbf{Graph oriented}. They represent information as the nodes of a graph and their relationships as edges, using graph theory to traverse it. Their strength is the analysis of the relationships between their objects and they represent hierarchical information very well, but they are not particularly good for scaling and tend to have a higher learning curve.
\end{itemize}

\section{Containers}

\nonzeroparskip Containerization is the packaging of code together with its dependencies, configurations and libraries to form a lightweight executable that can be executed in any infrastructure regardless of its system~\cite{ibm_containerization}. In this way, developers can focus on developing applications safely and quickly without worrying about subsequent execution, since the code they develop will be compiled into a package with all its dependencies, abstracting it from the operating system, isolating it and making it portable.

\nonzeroparskip The main advantages of containerization are summarized in:
\begin{itemize}
	\item \textbf{Portability}. The container's abstraction from the host operating system allows it to run consistently on any platform.
	\item \textbf{Agility}. The emergence of open source container engines like Docker has made it easier to integrate with DevOps elements and to run on different operating systems.
	\item \textbf{Speed}. Containers are light and fast to run due to their lack of an operating system and their limited content.
	\item \textbf{Fault isolation}. Each container is isolated and runs independently of the rest, so failures do not propagate between them.
	\item \textbf{Efficiency}. The container software shares the kernel of the operating system of the machine and the application layer can be shared between containers, making better use of system resources.
	\item \textbf{Ease of management}. Orchestrators make it extremely easy to install, manage, scale, and maintain containers, and the simplicity of containers also works in its favor.
	\item \textbf{Security}. Isolating containers acts as a security barrier against the spread of malware throughout the container environment.
	\end{itemize}

\nonzeroparskip A container is considerably lighter than a virtual machine, since it contains only an application and the elements necessary for its execution, while virtualization includes the entire operating system. The container has an engine to be executed and an orchestrator is usually used to manage several containers and their interconnections.

\section{Continuous Integration / Continuous Delivery}

\nonzeroparskip Continuous integration / continuous development or CI/CD is a software development and delivery method based on the introduction of automation in the stages of the development process, allowing work on iterables of the project subjected to testing phases. Continuous integration refers to the automation of development processes and building iterations, while continuous development refers to the continuous delivery of software and its deployment to the production environment~\cite{redhat_cicd}.

\nonzeroparskip A well-constructed CI/CD cycle helps developers merge new changes with the original project, as well as validate changes to ensure no new deficiencies or bugs are introduced into the product. Each functionality added to the main repository is tested both unitarily and functionally in an automated way, including these tests and allowing a quick analysis of possible conflicts before launching the new iteration of the product.

\section{Template engines}

\nonzeroparskip A template engine is software aimed at combining templates and data models to produce formatted output by rendering the data on the server side. The template engines transform the variables in the templates with the actual values and send them to the client~\cite{template_engines}.