\capitulo{3}{Theoretical concepts}

%Theoretical concepts of \LaTeX \footnote{Example of footnote}.
%\subsection{Subsection}
%\subsubsection{Subsubsection}
%Use of cite \cite{wiki:latex}, \cite{koza92}.
%\imagen{img/escudoInfor}{Image caption}
%Lists
%\begin{itemize}
%Enumerate.
%\begin{enumerate}
%Description.
%\begin{description}

\nonzeroparskip In this section are covered the theoretical concepts in which the project has been based. All concepts are described in a detailed and simple way since this master's thesis can be aimed at technical and non-technical students.

\section{Big Data}

\nonzeroparskip The term Big Data is used to refer to data sets whose size exceeds the capabilities of the software systems used to capture, preserve and process the data within an acceptable time window.

\nonzeroparskip The definition of Big Data is usually linked to the three dimensions or Vs defined by Doug Laney in 2001~\cite{bigdata_vs}:
\begin{itemize}
	\item \textbf{Volume.} The amount of data to handle.
	\item \textbf{Velocity.} The rate at which new data is created and consumed.
	\item \textbf{Variety.} The diversity or form of the captured data.
\end{itemize}

\nonzeroparskip After the definition of the three Vs, more dimensions have been added: Veracity, Value, Validity, Vagueness, Volatility... Up to a total of 42 different dimensions to define Big Data.

\nonzeroparskip From the moment big data is discovered until it can be used and generate value, it goes through a life cycle made up of the following phases: discovery, raw data ingestion, raw data processing, storage, integration with other data, analysis, and presentation of results.

\nonzeroparskip Due to the complexity of big data, it usually requires a complex and varied technological ecosystem to be managed.

\section{ETL}
\nonzeroparskip An ETL process refers to the stages that every data processing exercise usually goes through:
\begin{enumerate}
	\item \textbf{Extraction.} Raw data is collected from the source and fed into our Data Lake, which preserves it in its original form.
	\item \textbf{Transformation.} The raw data is cleaned and transformed to be used in our environment.
	\item \textbf{Load.} Once structured and filtered, the data is entered into our Data Warehouse.
\end{enumerate}

\section{API}

\nonzeroparskip An Application Programming Interface or API is an interface that defines the interactions that can be made with a software system. The APIs generally define the data that can be requested and sent to the system, the way to authenticate to it and the format of the returned data~\cite{ibm_restapi}.

\nonzeroparskip In relation to web development, most of the APIs work according to Hypertext Transfer Protocol (HTTP), a communication protocol that allows information transfers through files on the World Wide Web. Additionally and not exclusive, a large number of APIs are developed according to the REST architectural style, defined by Roy Fielding in the year 2000 and which is based on a series of principles that seek to facilitate development:
\begin{enumerate}
	\item Uniform interface for all resources, forcing all queries made to the same resource (each with a specific Uniform Resource Identifier or URI) to have the same form regardless of the origin of the request.
	\item Decoupling between the client and the server, making the only information that the client must know about the server is its identifier (URI) and that the only action to be carried out by the server is to return the data required in the request.
	\item Stateless queries, meaning that each request must contain all the information necessary to be processed without requiring an additional request or storing any type of state.
	\item Allow, whenever possible, both client-side and server-side caching to reduce the load of the former and increase the scalability of the latter.
	\item Layer system, allowing multiple intermediaries between the client and the server and preventing them from knowing in any case if they are communicating with the other party or with an intermediary.
	\item Although the resources exchanged are usually static, a REST architecture can optionally have responses that contain snippets of executable code.
\end{enumerate}

\nonzeroparskip In general terms, an API based on a REST architecture serves to make it easier for developers to develop applications that interact with the resources published by it.

\subsection{Twitter API}
\nonzeroparskip Twitter is an American social network founded in 2006 that allows users to share short posts (280 characters since 2017), known as tweets, and interact with those of other users through replies, likes, retweets or quotes~\cite{wikipedia_twitter}. Although it has recently incorporated additional payment functions, this social network is free to use and is accessible on multiple platforms. Currently, the social network has 217 million active users daily~\cite{variety_twitterusers}.

\nonzeroparskip On the other hand, Twitter is also known for giving certain facilities to developers to make their products interact with the platform. The company has a Twitter Developer portal where a multitude of resources and useful documentation are posted~\cite{twitter_dev}. This portal contains a description of the API that Twitter offers, how to authenticate, the different endpoints to which queries can be launched, and the associated usage limits. 

\nonzeroparskip The Twitter API, currently in its second version, allows the user to request and receive a wide variety of data. Depending on the query launched, the user can receive a series of different objects, each with its own fields and parameters:
\begin{itemize}
	\item \textbf{Tweets}. It represents the basic block of communication between Twitter users.
	\item \textbf{Users}. It represents a user account and its metadata.
	\item \textbf{Spaces}. It represents a space (virtual places in Twitter where users can interact in live conversations) and its metadata.
	\item \textbf{Lists}. It represents a Twitter list (used to configure information visualized in the timeline) and its metadata.
	\item \textbf{Media}. It represents any image, video or GIF attached to a tweet and can be obtained by expanding the Tweet object.
	\item \textbf{Polls}. It represents a poll (choices, duration, end-time and results) and can be obtained by expanding the Tweet object.
	\item \textbf{Places}. It represents a place identified in a tweet and can be obtained by expanding the Tweet object.
\end{itemize}

\nonzeroparskip Of all the endpoints available in the API (manage tweets, user lookup, search spaces, full-archive tweet search...), in this thesis only the Recent Search endpoint has been used, which returns a list of the most recent tweets based on the rules entered in the query. Both the number of tweets to receive and the id or date of the oldest tweet to be returned can be specified, and this endpoint allows receiving up to one hundred tweets per query and includes a pagination token to handle larger results~\cite{twitter_dev_searchtweets}.

\subsection{Spotify API}
\nonzeroparskip Spotify is a company of Swedish origin founded in 2006 that provides audio streaming services, currently being one of the companies with the largest number of users among all those that have this type of service. Spotify has a catalog made up of music and podcasts, including more than 82 million songs, distributed through a free service (limited control and periodic announcements) with the option of a premium subscription. Its business model is based on advertising and paying users, and it pays royalties to artists based on the proportion of streaming of their songs compared to the total played~\cite{wikipedia_spotify}.

\nonzeroparskip Spotify has a developer portal that provides a wealth of documentation to help design and implement various use cases. Spotify has an API based on a REST architecture with different published endpoints that return metadata of artists, albums and songs from its own catalog, as well as information on users, lists and music saved by them, in JSON format~\cite{spotify_dev}.

\nonzeroparskip The Spotify API has several endpoints to which queries can be sent to collect or modify information: Albums, Artists, Shows, Episodes, Search~\cite{spotify_dev_endpoint_searchforitem}, Tracks~\cite{spotify_dev_endpoint_gettracksaudiofeatures}, Users, Playlists, Categories, Genres, Player and Market. During this thesis the following have been used:
\begin{itemize}
	\item \textbf{Search}. Search for Item allows to obtain information about the Spotify catalog of artists, songs, albums, playlists, shows or episodes. You can specify the type or types of objects to return, in this case being the type ``tracks''. Only one search can be performed per query.
	\item \textbf{Tracks}. Get Tracks' Audio Features allows to obtain the characteristics of a set of songs specified by their id. The characteristics returned are as follows:
	\begin{itemize}
		\item \textbf{Acousticness}. Confidence measure from 0.0 to 1.0 about whether the track is acoustic, with 1.0 representing high confidence that it is acoustic.
		\item \textbf{Danceability}. It describes with a value between 0.0 and 1.0 how suitable a track is for dancing based on a combination of its musical elements, with 1.0 being the greatest danceability.
		\item \textbf{Duration\_ms}. It represents the duration of the track in milliseconds.
		\item \textbf{Energy}. It represents with a value between 0.0 and 1.0 the conception of the energy level of the track, being 1.0 the maximum energy value.
		\item \textbf{Instrumentalness}. It predicts with a value between 0.0 and 1.0 whether or not the track contains vocals. Values above 0.5 usually represent tracks without vocals, and the closer to 1.0 the more likely they are.
		\item \textbf{Key}. It indicates the key (in a musical context, the dominant scale) the track is in, with each key having an assigned integer starting with 0. If no key is detected, the value is -1.
		\item \textbf{Liveness}. It represents audience presence with a value between 0.0 and 1.0. Values greater than 0.8 indicate a high probability that the track was recorded live.
		\item \textbf{Loudness}. Indicates the average volume of a track in decibels, with values generally contained between -60dB and 0dB.
		\item \textbf{Mode}. Indicates the modality of the track, being the value 1 greater and 0 less.
		\item \textbf{Speechiness}. Detects the presence of spoken words in a track. Values less than 0.33 typically indicate instrumental tracks without vocals, values between 0.33 and 0.66 songs with music and vocals, and values greater than 0.66 podcasts, audiobooks, and similar formats.
		\item \textbf{Tempo}. Indicates the tempo or rhythm of a track in beats per minute.
		\item \textbf{Time\_signature}. Represents the estimated time signature value, with values between 3 and 7 indicating 3/4 and 7/4 time signatures, respectively.
		\item \textbf{Valence}. Indicates with values between 0.0 and 1.0 the musical positivity transmitted by the track, where high values indicate greater positivity, while low values indicate greater negativity.
	\end{itemize}
\end{itemize}

\section{Orchestrator}

\nonzeroparskip The great variety of applications and services that exist in technological environments, where there are workflows with various actors with interdependencies between them, make their management and automation enormously complex. The more complex a system is, the more difficult it is to manage the intervening factors~\cite{redhat_orchestrator}.

\nonzeroparskip System automation usually improves efficiency, simplifies management, and reduces associated costs, both in terms of time spent and personnel required to control it. On the other hand, a distinction is made between automation and orchestration in that the former refers to a single task, while the latter comprises multi-step processes and workflows, being the scope of work of the orchestrators.

\nonzeroparskip Two main orchestrators have been used in this project: Docker Compose, which acts as an orchestrator for the work environment containers, and Apache Airflow, which orchestrates the project's workflow.

\section{NoSQL Databases}

\nonzeroparskip A database is a set of data belonging to the same context and stored for later use, and can be updated periodically. The best known type of databases are relational databases. In a relational database, the data attributes are stored in the form of columns, previously defined, and the values are stored in the rows of the table for all its columns or attributes. These databases have an associated query language called SQL (Structured Query Language).

\nonzeroparskip Relational database properties are summarized in ACID properties:
\begin{itemize}
	\item \textbf{A}tomicity. The process is done completely or it is not done.
	\item \textbf{C}onsistency. Only valid data is written.
	\item \textbf{I}solation. The operations are performed one at a time.
	\item \textbf{D}urability. When an operation is performed, it persists and is not undone even if the system crashes.
\end{itemize}

\nonzeroparskip However, in the face of the massive volumes of data that are associated with the concept of Big Data, relational databases have a series of limitations:
\begin{itemize}
	\item Reading the data is expensive, since the data is represented in tables, queries involve joining large data sets and filtering the results.
	\item The stored information usually has similar structures, a concept that does not agree well with Big Data, where the variety of data structure is greater.
	\item Scalability is not their strongest factor, since they were initially designed considering a single server or, at most, having replicas and load balancing.
\end{itemize}

\nonzeroparskip Distributed databases are limited by the CAP theorem:
\begin{itemize}
	\item \textbf{C}onsistency. The information remains coherent and consistent after any operation, with all copies having the same data at all times.
	\item \textbf{A}vailability. The system continues to function even if any of its nodes or parts of the software or hardware fail, and all reads and writes complete successfully.
	\item \textbf{P}artition tolerance. The system nodes will continue to function even if the connection between them fails or messages are lost, maintaining their properties.
\end{itemize}

\nonzeroparskip According to CAP's theorem, any distributed database with shared data among its nodes can have at most two of the three properties at the same time. This theorem resulted in databases with relaxed ACID properties, that is, with BASE properties:
\begin{itemize}
	\item \textbf{B}asically \textbf{A}vailable: The store works most of the time, even if failures occur.
	\item \textbf{S}oft-State: Stores or their replicas do not have to be consistent at all times.
	\item \textbf{E}ventually Consistent: consistency happens eventually, as it is something that is taken for granted at some point in the future. All copies will gradually become consistent if no further updates are run.
\end{itemize}

\nonzeroparskip Non-relational databases or NoSQL (Not Only SQL), a term introduced by Carl Strozzi in 1998 that describes all those databases that do not follow the same design patterns as relational databases. Non-relational databases follow the BASE properties and have advantages such as:
\begin{itemize}
	\item They do not require a fixed data schema.
	\item The data is replicated on multiple similar nodes and can be partitioned.
	\item They are horizontally scalable, that is, by adding new nodes.
	\item They are relatively inexpensive and simple to implement, with a host of open source alternatives.
	\item They provide fast read and write speeds, with fast key-value access.
\end{itemize}

\nonzeroparskip However, the main disadvantages of non-relational databases is that they do not support certain features of relational databases (join, group by, order by...) except within their partitions, they do not have a query language standard such as SQL and its relaxed ACID or BASE properties give lesser guarantees.

\nonzeroparskip Non-relational databases are mainly divided into four groups:
\begin{itemize}
	\item \textbf{Key/value}. Their data model is very simple, since they only store keys and values. They are very similar to a hash table, they are fast, they have great ease of scaling, eventual consistency and fault tolerance, although they cannot support complex data structures.
	\item \textbf{Column oriented}. Data is stored in columns instead of rows. The data is semi-structured, easily distributable, provides high reading speed, calculations on attributes are faster (especially aggregations such as averages) and are perfect when you want to do many operations on large data sets, but they are not the most efficient for writing or when you want to retrieve all records. The data model has columns, super columns, column families, and super column families.
	\item \textbf{Document oriented}. These are key-value stores in which the value is stored as a document with a defined format, so the final data model is collections of documents with a key-value structure (JSON, PDF, XML...). They are schema-less, highly scalable, programmer-friendly, and support rapid development.
	\item \textbf{Graph oriented}. They represent information as the nodes of a graph and their relationships as edges, using graph theory to traverse it. Their strength is the analysis of the relationships between their objects and they represent hierarchical information very well, but they are not particularly good for scaling and tend to have a higher learning curve.
\end{itemize}

\section{Containers}

\nonzeroparskip Containerization is the packaging of code together with its dependencies, configurations and libraries to form a lightweight executable that can be executed in any infrastructure regardless of its system~\cite{ibm_containerization}. In this way, developers can focus on developing applications safely and quickly without worrying about subsequent execution, since the code they develop will be compiled into a package with all its dependencies, abstracting it from the operating system, isolating it and making it portable.

\nonzeroparskip The main advantages of containerization are summarized in:
\begin{itemize}
	\item \textbf{Portability}. The container's abstraction from the host operating system allows it to run consistently on any platform.
	\item \textbf{Agility}. The emergence of open source container engines like Docker has made it easier to integrate with DevOps elements and to run on different operating systems.
	\item \textbf{Speed}. Containers are light and fast to run due to their lack of an operating system and their limited content.
	\item \textbf{Fault isolation}. Each container is isolated and runs independently of the rest, so failures do not propagate between them.
	\item \textbf{Efficiency}. The container software shares the kernel of the operating system of the machine and the application layer can be shared between containers, making better use of system resources.
	\item \textbf{Ease of management}. Orchestrators make it extremely easy to install, manage, scale, and maintain containers, and the simplicity of containers also works in its favor.
	\item \textbf{Security}. Isolating containers acts as a security barrier against the spread of malware throughout the container environment.
	\end{itemize}

\nonzeroparskip A container is considerably lighter than a virtual machine, since it contains only an application and the elements necessary for its execution, while virtualization includes the entire operating system. The container has an engine to be executed and an orchestrator is usually used to manage several containers and their interconnections.

\section{Continuous Integration / Continuous Delivery}

\nonzeroparskip Continuous integration / continuous development or CI/CD is a software development and delivery method based on the introduction of automation in the stages of the development process, allowing work on iterables of the project subjected to testing phases. Continuous integration refers to the automation of development processes and building iterations, while continuous development refers to the continuous delivery of software and its deployment to the production environment~\cite{redhat_cicd}.

\nonzeroparskip A well-constructed CI/CD cycle helps developers merge new changes with the original project, as well as validate changes to ensure no new deficiencies or bugs are introduced into the product. Each functionality added to the main repository is tested both unitarily and functionally in an automated way, including these tests and allowing a quick analysis of possible conflicts before launching the new iteration of the product.

\section{Template engines}

\nonzeroparskip Section explaining Template engines -> Jinja.

\section{Web Server Gateway Interface}

\nonzeroparskip Section explaining Web Server Gateway Interface (WSGI).